\relax 
\providecommand\hyper@newdestlabel[2]{}
\bibstyle{mdpi}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{vargas2019model,lawani2019reviews,shen2021information}
\citation{vargas2019model,lawani2019reviews,shen2021information}
\citation{vargas2019model}
\citation{lawani2019reviews}
\citation{shen2021information}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\newlabel{introduction}{{1}{1}{Introduction}{section.1}{}}
\citation{streeteasy_2019}
\citation{zipcodeR_2020}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Related Work}{2}{subsection.1.1}}
\newlabel{related-work}{{1.1}{2}{Related Work}{subsection.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Research Questions and Objectives}{2}{subsection.1.2}}
\newlabel{research-questions-and-objectives}{{1.2}{2}{Research Questions and Objectives}{subsection.1.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Methods}{2}{section.2}}
\newlabel{methods}{{2}{2}{Methods}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Data and Variables}{2}{subsection.2.1}}
\newlabel{data-and-variables}{{2.1}{2}{Data and Variables}{subsection.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Data Preprocessing}{2}{subsection.2.2}}
\newlabel{data-preprocessing}{{2.2}{2}{Data Preprocessing}{subsection.2.2}{}}
\citation{mice_2011}
\citation{caret2020}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Summary statistics of numerical variables.\relax }}{3}{table.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{numDescriptive}{{1}{3}{Summary statistics of numerical variables.\relax }{table.caption.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Summary statistics of categorical variables. More than 80\% of the data are listings in New York City, and the listings are mainly residential units. Please see Appendix A for information about the city\_group variable. \relax }}{4}{table.caption.2}}
\newlabel{catDescriptive}{{2}{4}{Summary statistics of categorical variables. More than 80\% of the data are listings in New York City, and the listings are mainly residential units. Please see Appendix A for information about the city\_group variable. \relax }{table.caption.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces An example listing with its physical attributes and text description, transposed from a row in our data. It is a co-op in Manhattan that has 2 bedrooms, 1 bathroom, and 900 square feet. The closing price of this listing is \$1,175,000.\relax }}{5}{table.caption.3}}
\newlabel{tab:sampleListing}{{3}{5}{An example listing with its physical attributes and text description, transposed from a row in our data. It is a co-op in Manhattan that has 2 bedrooms, 1 bathroom, and 900 square feet. The closing price of this listing is \$1,175,000.\relax }{table.caption.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces List of predictor variables for the baseline model without text-based features.\relax }}{6}{table.caption.4}}
\newlabel{tab:noTextVar}{{4}{6}{List of predictor variables for the baseline model without text-based features.\relax }{table.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Data Analysis}{6}{subsection.2.3}}
\newlabel{data-analysis}{{2.3}{6}{Data Analysis}{subsection.2.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1}Random Forest Model}{6}{subsubsection.2.3.1}}
\newlabel{random-forest-model}{{2.3.1}{6}{Random Forest Model}{subsubsection.2.3.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2}Text Analysis}{6}{subsubsection.2.3.2}}
\newlabel{text-analysis}{{2.3.2}{6}{Text Analysis}{subsubsection.2.3.2}{}}
\citation{nielsen2011new}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Sentiment scores assigned to words in the example listing description. This listing has a total sentiment score of 20 by mentioning the words in the table.\relax }}{8}{table.caption.5}}
\newlabel{tab:samplescore}{{5}{8}{Sentiment scores assigned to words in the example listing description. This listing has a total sentiment score of 20 by mentioning the words in the table.\relax }{table.caption.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Comparison of MAPE and RMSE when variables are sequentially added in 100-tree random forest models.\relax }}{8}{table.caption.6}}
\newlabel{tab:improvement}{{6}{8}{Comparison of MAPE and RMSE when variables are sequentially added in 100-tree random forest models.\relax }{table.caption.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Results}{8}{section.3}}
\newlabel{results}{{3}{8}{Results}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Models without Text Features}{8}{subsection.3.1}}
\newlabel{models-without-text-features}{{3.1}{8}{Models without Text Features}{subsection.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Text Analysis}{8}{subsection.3.2}}
\newlabel{text-analysis-1}{{3.2}{8}{Text Analysis}{subsection.3.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Unigram and bigram analyses}{8}{subsubsection.3.2.1}}
\newlabel{unigram-and-bigram-analyses}{{3.2.1}{8}{Unigram and bigram analyses}{subsubsection.3.2.1}{}}
\citation{kuhn2012variable}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Random Forest Models with Text-based Variables}{9}{subsection.3.3}}
\newlabel{random-forest-models-with-text-based-variables}{{3.3}{9}{Random Forest Models with Text-based Variables}{subsection.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Sentimental Score and Word Count}{9}{subsection.3.4}}
\newlabel{sentimental-score-and-word-count}{{3.4}{9}{Sentimental Score and Word Count}{subsection.3.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Error Analysis}{9}{subsection.3.5}}
\newlabel{error-analysis}{{3.5}{9}{Error Analysis}{subsection.3.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Top 50 most frequent words from unigram analysis. Among the most frequent words are bedroom, kitchen, home, and living. While these words are not particularly informative in creating binary variables, other words such as storage, private, and park are used to inform binary variable creation.\relax }}{10}{figure.caption.7}}
\newlabel{fig:unigram}{{1}{10}{Top 50 most frequent words from unigram analysis. Among the most frequent words are bedroom, kitchen, home, and living. While these words are not particularly informative in creating binary variables, other words such as storage, private, and park are used to inform binary variable creation.\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Top 50 most frequent two-word phrases from bigram analysis. Among the most frequent bigrams are stainless steel, hardwood floors, washer and dryer, steel appliances, and fitness center. These bigrams are used to inform binary variable creation.\relax }}{11}{figure.caption.8}}
\newlabel{fig:bigram}{{2}{11}{Top 50 most frequent two-word phrases from bigram analysis. Among the most frequent bigrams are stainless steel, hardwood floors, washer and dryer, steel appliances, and fitness center. These bigrams are used to inform binary variable creation.\relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Top 40 frequent words or bigrams and the average price of the listings that mention these words. A lighter shade of blue indicates that the word or phrase is associated with a higher average number of bathrooms. A larger circle indicates that the word or phrase is associated with a higher average number of bedrooms. Words and phrases that are associated with relatively high prices are used to inform binary variable creation.\relax }}{12}{figure.caption.9}}
\newlabel{fig:wordprice}{{3}{12}{Top 40 frequent words or bigrams and the average price of the listings that mention these words. A lighter shade of blue indicates that the word or phrase is associated with a higher average number of bathrooms. A larger circle indicates that the word or phrase is associated with a higher average number of bedrooms. Words and phrases that are associated with relatively high prices are used to inform binary variable creation.\relax }{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Variable importance plot of the optimal random forest model with text-based variables. Of the top twenty most important variables, size in square feet is most important with a score of 100. In general, text-based variables have importance scores less than 25 but do help improve model performance.\relax }}{13}{figure.caption.10}}
\newlabel{fig:varImpPlot}{{4}{13}{Variable importance plot of the optimal random forest model with text-based variables. Of the top twenty most important variables, size in square feet is most important with a score of 100. In general, text-based variables have importance scores less than 25 but do help improve model performance.\relax }{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Relationship between sentimental score and word count of listing descriptions. The scatterplot reveals that as word count increases, so does sentiment score as is reflected by the blue trend line.\relax }}{13}{figure.caption.11}}
\newlabel{fig:regression}{{5}{13}{Relationship between sentimental score and word count of listing descriptions. The scatterplot reveals that as word count increases, so does sentiment score as is reflected by the blue trend line.\relax }{figure.caption.11}{}}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Regression Output of Word Count and Sentiment Score. In this regression table, p < 0.001 which is less than the threshold value of 0.05, indicating a significant relationship between listing descriptions\IeC {\textquoteright } word count and sentiment score.\relax }}{14}{table.caption.12}}
\newlabel{regressionTable}{{7}{14}{Regression Output of Word Count and Sentiment Score. In this regression table, p < 0.001 which is less than the threshold value of 0.05, indicating a significant relationship between listing descriptions’ word count and sentiment score.\relax }{table.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Relationship between home prices and prediction errors greater than third quartile. This plot only includes the least accurate predictions of test data, which overestimate or underestimate the listing prices by more than \$301,749. On the x-axis is the listing price and on the y-axis is the prediction error calculated by subtracting the actual price from the predicted price. The grey vertical line is the mean listing price in the test data. The plot shows that listing prices greater than \$3,500,000 always get underestimated and listing prices lower than \$500,000 always get overestimated.\relax }}{14}{figure.caption.13}}
\newlabel{fig:plotError}{{6}{14}{Relationship between home prices and prediction errors greater than third quartile. This plot only includes the least accurate predictions of test data, which overestimate or underestimate the listing prices by more than \$301,749. On the x-axis is the listing price and on the y-axis is the prediction error calculated by subtracting the actual price from the predicted price. The grey vertical line is the mean listing price in the test data. The plot shows that listing prices greater than \$3,500,000 always get underestimated and listing prices lower than \$500,000 always get overestimated.\relax }{figure.caption.13}{}}
\citation{vargas2019model}
\@writefile{toc}{\contentsline {section}{\numberline {4}Discussion}{15}{section.4}}
\newlabel{discussion}{{4}{15}{Discussion}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Conclusion}{15}{subsection.4.1}}
\newlabel{conclusion}{{4.1}{15}{Conclusion}{subsection.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Limitations}{15}{subsection.4.2}}
\newlabel{limitations}{{4.2}{15}{Limitations}{subsection.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Future Directions}{15}{subsection.4.3}}
\newlabel{future-directions}{{4.3}{15}{Future Directions}{subsection.4.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Ethics Statement}{16}{subsection.4.4}}
\newlabel{ethics-statement}{{4.4}{16}{Ethics Statement}{subsection.4.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A}Supplementary Tables}{17}{section.A.}}
\@writefile{lot}{\contentsline {table}{\numberline {A1}{\ignorespaces Variables in the original data set provided by StreetEasy\relax }}{17}{table.caption.14}}
\newlabel{intial_var}{{A1}{17}{Variables in the original data set provided by StreetEasy\relax }{table.caption.14}{}}
\@writefile{lot}{\contentsline {table}{\numberline {A2}{\ignorespaces Cities categorized by its median listing price.\relax }}{17}{table.2}}
\newlabel{citygroup}{{A2}{17}{Cities categorized by its median listing price.\relax }{table.2}{}}
\gdef \LT@i {\LT@entry 
    {1}{212.79999pt}\LT@entry 
    {1}{132.4pt}\LT@entry 
    {1}{62.4pt}}
\@writefile{lot}{\contentsline {table}{\numberline {A3}{\ignorespaces List of text-based variables.\relax }}{20}{table.caption.15}}
\newlabel{tab:textVar}{{A3}{20}{List of text-based variables.\relax }{table.caption.15}{}}
\@writefile{lot}{\contentsline {table}{\numberline {A4}{\ignorespaces Summary statistics of binary text-based variables\relax }}{21}{table.caption.16}}
\newlabel{binary}{{A4}{21}{Summary statistics of binary text-based variables\relax }{table.caption.16}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B}Ethics Statement}{21}{section.B.}}
\citation{boeing2021housing}
\bibdata{mybibfile.bib}
\bibcite{vargas2019model}{{1}{2019}{{Vargas-Calder{\'o}n and Camargo}}{{}}}
\bibcite{lawani2019reviews}{{2}{2019}{{Lawani \em  {et~al.}}}{{Lawani, Reed, Mark, and Zheng}}}
\bibcite{shen2021information}{{3}{2021}{{Shen and Ross}}{{}}}
\bibcite{streeteasy_2019}{{4}{2019}{{str}}{{}}}
\bibcite{zipcodeR_2020}{{5}{2020}{{Rozzi}}{{}}}
\bibcite{mice_2011}{{6}{2011}{{{van Buuren} and Groothuis-Oudshoorn}}{{}}}
\bibcite{caret2020}{{7}{2020}{{Kuhn}}{{}}}
\bibcite{nielsen2011new}{{8}{2011}{{Nielsen}}{{}}}
\bibcite{kuhn2012variable}{{9}{2012}{{Kuhn}}{{}}}
\bibcite{boeing2021housing}{{10}{2021}{{Boeing \em  {et~al.}}}{{Boeing, Besbris, Schachter, and Kuk}}}
\@writefile{toc}{\contentsline {section}{References}{23}{section.B.}}
\newlabel{LastPage}{{}{23}{}{page.23}{}}
\xdef\lastpage@lastpage{23}
\xdef\lastpage@lastpageHy{23}
\expandafter\ifx\csname c@page@totc\endcsname\relax\newcounter{page@totc}\fi\setcounter{page@totc}{24}
